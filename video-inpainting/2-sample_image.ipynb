{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, \"../sd-scripts\")\n",
    "import torch\n",
    "from library import train_util, sdxl_train_util, sdxl_model_util\n",
    "from library.sdxl_lpw_stable_diffusion import (\n",
    "    SdxlStableDiffusionLongPromptWeightingPipeline,\n",
    ")\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "from library.video_inpainting_patch import (\n",
    "    VideoInpaintingPatchPipeline,\n",
    "    VideoInpaintingPatch,\n",
    ")\n",
    "from networks import lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import train_network\n",
    "\n",
    "\n",
    "def setup_parser() -> argparse.ArgumentParser:\n",
    "    parser = train_network.setup_parser()\n",
    "    sdxl_train_util.add_sdxl_training_arguments(parser)\n",
    "    return parser\n",
    "\n",
    "\n",
    "parser = setup_parser()\n",
    "\n",
    "argv = [\n",
    "    \"--config_file\",\n",
    "    \"/home/longc/data/code/lora-scripts/config/video-example/video-debug.toml\",\n",
    "]\n",
    "args = parser.parse_args(argv)\n",
    "args = train_util.read_config_from_file(args, parser, argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preparing accelerator\")\n",
    "accelerator = train_util.prepare_accelerator(args)\n",
    "is_main_process = accelerator.is_main_process\n",
    "\n",
    "# mixed precisionに対応した型を用意しておき適宜castする\n",
    "weight_dtype, _ = train_util.prepare_dtype(args)\n",
    "device = accelerator.device\n",
    "# device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "if device.type == \"cpu\":\n",
    "    weight_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = sdxl_train_util.load_tokenizers(args)\n",
    "if not isinstance(tokenizers, list):\n",
    "    tokenizers = [tokenizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    load_stable_diffusion_format,\n",
    "    text_encoder1,\n",
    "    text_encoder2,\n",
    "    vae,\n",
    "    unet,\n",
    "    logit_scale,\n",
    "    ckpt_info,\n",
    ") = sdxl_train_util.load_target_model(\n",
    "    args, accelerator, sdxl_model_util.MODEL_VERSION_SDXL_BASE_V1_0, weight_dtype\n",
    ")\n",
    "\n",
    "text_encoders = [text_encoder1, text_encoder2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if device.type == \"cuda\":\n",
    "    # xformers memory efficient attention\n",
    "    train_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdpa)\n",
    "    if torch.__version__ >= \"2.0.0\":\n",
    "        vae.set_use_memory_efficient_attention_xformers(args.xformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_file = \"/home/longc/data/code/lora-scripts/output/video-debug2/video-debug-step00100000.safetensors\"\n",
    "multiplier = 1.0\n",
    "lora_network, weights_sd = lora.create_network_from_weights(\n",
    "    multiplier, lora_file, vae, text_encoders, unet, for_inference=True\n",
    ")\n",
    "lora_network.merge_to(\n",
    "    text_encoders, unet, weights_sd, weight_dtype, device if args.lowram else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.to(device, dtype=weight_dtype).eval()\n",
    "for t_enc in text_encoders:\n",
    "    t_enc.to(device, dtype=weight_dtype).eval()\n",
    "\n",
    "vae = vae.to(device, dtype=weight_dtype).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_file = \"/home/longc/data/code/lora-scripts/output/video-debug2/video-debug-step00100000_inpainting_head.pth\"\n",
    "\n",
    "# load inpainting head\n",
    "inpainting_head = VideoInpaintingPatch(sdxl_model_util.VAE_SCALE_FACTOR).to(device)\n",
    "inpainting_head.load_state_dict(torch.load(patch_file, map_location=\"cpu\"))\n",
    "inpainting_head.to(device, dtype=weight_dtype).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = {\n",
    "    \"prompt\": \"(masterpiece, best quality:1.2), a person preparing a pizza on a table in a kitchen,\",\n",
    "    \"negative_prompt\": \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, \",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 576,\n",
    "    \"scale\": 4.0,\n",
    "    \"strength\": 1,\n",
    "    \"sample_steps\": 50,\n",
    "    \"seed\": 1337,\n",
    "    \"image\": \"/home/longc/data/code/lora-scripts/config/video-example/images/00006154.jpg\",\n",
    "    \"mask\": \"/home/longc/data/code/lora-scripts/config/video-example/masks/00006154.png\",\n",
    "    \"prev_image\": \"/home/longc/data/code/lora-scripts/video-inpainting/output/video-debug/sample/video-debug_20240124104531_e000000_00_1337.png\",\n",
    "    \"prev_mask\": \"/home/longc/data/code/lora-scripts/config/video-example/masks/00006152.png\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompts(data_dir, max_size=1024):\n",
    "    data_dir = Path(data_dir)\n",
    "    images = sorted(data_dir.glob(\"images/*.jpg\"))\n",
    "    masks = sorted(data_dir.glob(\"masks/*.png\"))\n",
    "    captions = sorted(data_dir.glob(\"caption/*.txt\"))\n",
    "    n_images = len(images)\n",
    "    for prev, curr in zip(range(0, n_images - 1), range(1, n_images)):\n",
    "        with open(captions[curr], \"r\") as f:\n",
    "            caption = f.read()\n",
    "        image = Image.open(images[curr])\n",
    "        w, h = image.width, image.height\n",
    "        scale = min(max_size / w, max_size / h)\n",
    "        if scale < 1:\n",
    "            w = int(w * scale)\n",
    "            h = int(h  * scale)\n",
    "            \n",
    "        prompt = copy.copy(PROMPT_TEMPLATE)\n",
    "        prompt.update(\n",
    "            {\n",
    "                \"prompt\": caption,\n",
    "                \"width\": w,\n",
    "                \"height\": h,\n",
    "                \"image\": str(images[curr]),\n",
    "                \"mask\": str(masks[curr]),\n",
    "                \"prev_image\": str(images[prev]),\n",
    "                \"prev_mask\": str(masks[prev]),\n",
    "            }\n",
    "        )\n",
    "        yield prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(create_prompts(\"/home/longc/data/code/lora-scripts/config/video-example-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir = \"./output/video-debug-iterate7\"\n",
    "args.sample_sampler = \"euler_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(prompts)), total=len(prompts)):\n",
    "    generated_files = train_util.sample_images_common(\n",
    "        partial(VideoInpaintingPatchPipeline, inpainting_head=inpainting_head),\n",
    "        accelerator,\n",
    "        args,\n",
    "        epoch=0,\n",
    "        steps=0,\n",
    "        device=device,\n",
    "        vae=vae,\n",
    "        tokenizer=tokenizers,\n",
    "        text_encoder=text_encoders,\n",
    "        unet=unet,\n",
    "        prompts_data=prompts[i : i + 1],\n",
    "        verbose=False\n",
    "    )\n",
    "    # if i + 1 < len(prompts):\n",
    "    #     prompts[i + 1][\"prev_image\"] = generated_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
